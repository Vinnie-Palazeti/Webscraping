{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Barstool.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjMs-Yb51Jar"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from datetime import date\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import unicodedata\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwldWXWe1UOL"
      },
      "source": [
        "# creating url iterator \r\n",
        "start_date = date(2019, 1, 1)\r\n",
        "end_date = date(2020, 1, 1)\r\n",
        "daterange = pd.date_range(start_date, end_date)\r\n",
        "\r\n",
        "dates = pd.DataFrame({\"dates\":daterange})\r\n",
        "dates = dates.iloc[::5, :].reset_index(drop=True)\r\n",
        "\r\n",
        "after = dates.iloc[::2].reset_index(drop=True)  # even\r\n",
        "before = dates.iloc[1::2].reset_index(drop=True)  # odd\r\n",
        "\r\n",
        "# the way their html functions requires overlapping date sets\r\n",
        "after_offset = after + pd.DateOffset(1)\r\n",
        "before_offset = before + pd.DateOffset(-1)\r\n",
        "\r\n",
        "# format\r\n",
        "after = after.dates.dt.strftime('%Y-%m-%d')\r\n",
        "before = before.dates.dt.strftime('%Y-%m-%d')\r\n",
        "after_offset = after_offset.dates.dt.strftime('%Y-%m-%d')\r\n",
        "before_offset = before_offset.dates.dt.strftime('%Y-%m-%d')\r\n",
        "\r\n",
        "# create date list\r\n",
        "urls = []\r\n",
        "for j in range(0,36):\r\n",
        "  url_1 = \"https://union.barstoolsports.com/stories/latest?limit=1500&after=\"+ after[j] + \"&before=\" + before[j] +\"&type=standard_post\"\r\n",
        "  urls.append(url_1)\r\n",
        "  url_2 = \"https://union.barstoolsports.com/stories/latest?limit=1500&after=\"+ before_offset[j] + \"&before=\" + after_offset[j+1] +\"&type=standard_post\"\r\n",
        "  urls.append(url_2)\r\n",
        "\r\n",
        "# empty dataframe to append\r\n",
        "Barstool = pd.DataFrame({\r\n",
        "        \"dates\":[] ,\r\n",
        "        \"author\": [],\r\n",
        "        \"title\": [],\r\n",
        "        \"text\": [],\r\n",
        "        \"comments\": [],\r\n",
        "        \"blockquotes\": [],\r\n",
        "        \"twitter\": [],\r\n",
        "        \"instagram\": [],\r\n",
        "        \"tiktok\": [],\r\n",
        "        \"youtube\": [],\r\n",
        "        \"images\": [],\r\n",
        "        \"streamable\": [],\r\n",
        "        'gifs': []\r\n",
        "        })\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NdUf5_v1cQS"
      },
      "source": [
        "for i in range(len(urls)):\r\n",
        "  # read in directory\r\n",
        "  info = pd.read_json(urls[i])['url']\r\n",
        "  print(\"---\", i, \"---\")\r\n",
        "  # each directory has hundreds of blogs in chronological order\r\n",
        "  for k in range(len(info)):\r\n",
        "    try:\r\n",
        "      print(\"---\", k,\"---\")\r\n",
        "      url = info[k]\r\n",
        "      page = requests.get(url)\r\n",
        "      soup = BeautifulSoup(page.content,\"html.parser\")\r\n",
        "      words = [t.get_text() for t in soup.select(\".story__content\")][0]\r\n",
        "      words = unicodedata.normalize('NFKD',words)\r\n",
        "      author = [w.get_text() for w in soup.select(\".authorName\")][0]\r\n",
        "      date = [d.get_text() for d in soup.select(\".timestamp\")][0]\r\n",
        "      title = [p.get_text() for p in soup.select(\".story__title\")][0]\r\n",
        "      comments = len(soup.find_all(\"span\", { \"class\":\"jsx-3955762224\" }))\r\n",
        "      links = soup.select('.iframely-embed')\r\n",
        "      links = [i.findChild(\"a\")['href'] for i in links]\r\n",
        "      twitter = len([s for s in links if \"twitter.com\" in s])\r\n",
        "      insta = len([s for s in links if \"instagram.com\" in s])\r\n",
        "      tiktok = len([s for s in links if \"tiktok.com\" in s])\r\n",
        "      streamable = len([s for s in links if \"streamable.com\" in s])\r\n",
        "      youtube = len([s for s in links if \"youtube.com\" in s])\r\n",
        "      imgs = len(soup.findAll(\"img\", {\"class\": \"fr-fic fr-dib bs-image\"}))\r\n",
        "      imgs = imgs + len(soup.findAll(\"img\",{'class':'alignnone'}))\r\n",
        "      blockquotes = len(soup.select('blockquote'))\r\n",
        "      gifs = len(soup.find_all(\"video\"))\r\n",
        "\r\n",
        "      frame = pd.DataFrame({\r\n",
        "            \"dates\": date,\r\n",
        "            \"author\": author,\r\n",
        "            \"title\": title,\r\n",
        "            \"text\": words,\r\n",
        "            \"comments\": [comments],\r\n",
        "            \"blockquotes\": [blockquotes],\r\n",
        "            \"twitter\": [twitter],\r\n",
        "            \"instagram\": [insta],\r\n",
        "            \"tiktok\": [tiktok],\r\n",
        "            \"youtube\": [youtube],\r\n",
        "            \"images\": [imgs],\r\n",
        "            \"streamable\": [streamable],\r\n",
        "            'gifs': [gifs]\r\n",
        "            })\r\n",
        "    \r\n",
        "      Barstool = Barstool.append(frame)\r\n",
        "    except:\r\n",
        "      print(\"Barstool Booted me\")\r\n",
        "      print(\"Or some other error\")\r\n",
        "      print(\"Waiting 30 seconds...\")\r\n",
        "      time.sleep(30)\r\n",
        "      continue\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4VHXxTVFbjp",
        "outputId": "3e6ea755-d784-459f-832c-b2a876ea1d84"
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xlsxwriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/41/bf1aae04932d1eaffee1fc5f8b38ca47bbbf07d765129539bc4bcce1ce0c/XlsxWriter-1.3.7-py2.py3-none-any.whl (144kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20kB 20.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 30kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 51kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 61kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 71kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 81kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 92kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 102kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 112kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 122kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 133kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 143kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 13.6MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-1.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fa-k46vGpcj"
      },
      "source": [
        "import pickle\n",
        "with open('./Barstool_2019.pkl', 'wb') as fid:\n",
        "     pickle.dump(Barstool, fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbRdt5J5Espr"
      },
      "source": [
        "Barstool.to_excel('Barstool_2019.xlsx', engine='xlsxwriter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwUas3niGKF1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}